{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "import math\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_json = pd.read_json('Income.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of kids</th>\n",
       "      <th>Income</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>122500</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>142007</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>42007</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14704</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200704</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>120070</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>207040</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>48000</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>79000</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # of kids  Income State\n",
       "0          5   25000    CA\n",
       "1          5  122500    NY\n",
       "2          2  142007    TX\n",
       "3          2   42007    TX\n",
       "4          0   14704    TX\n",
       "5          1  200704    TX\n",
       "6          1  120070    CA\n",
       "7          3  207040    NY\n",
       "8          3   48000    NY\n",
       "9          3   79000    NY"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(income_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercasing across the dataframe\n",
    "\n",
    "income_json = income_json.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the columns\n",
    "\n",
    "income_json.columns = map(str.lower, income_json.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># of kids</th>\n",
       "      <th>income</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>122500</td>\n",
       "      <td>ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>142007</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>42007</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14704</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>200704</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>120070</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>207040</td>\n",
       "      <td>ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>48000</td>\n",
       "      <td>ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>79000</td>\n",
       "      <td>ny</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  # of kids  income state\n",
       "0         5   25000    ca\n",
       "1         5  122500    ny\n",
       "2         2  142007    tx\n",
       "3         2   42007    tx\n",
       "4         0   14704    tx\n",
       "5         1  200704    tx\n",
       "6         1  120070    ca\n",
       "7         3  207040    ny\n",
       "8         3   48000    ny\n",
       "9         3   79000    ny"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation, though this is redundant since the data contains\n",
    "# no punctuation to begin with.\n",
    "\n",
    "income_json = income_json.apply(lambda x: x.astype(str).replace('[^\\w\\s]',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tdrace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words. Since there are no stop words in the data (outside of\n",
    "# the column names), this too is redundant, but here's how you can do it.\n",
    "\n",
    "income_json['state'] = income_json['state'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PorterStemmer. Again, there is no true data to use this on, the state\n",
    "# names are just abbrevations, but here's one way to do it.\n",
    "\n",
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = income_json['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca  :  ca\n",
      "ny  :  ny\n",
      "tx  :  tx\n",
      "tx  :  tx\n",
      "tx  :  tx\n",
      "tx  :  tx\n",
      "ca  :  ca\n",
      "ny  :  ny\n",
      "ny  :  ny\n",
      "ny  :  ny\n"
     ]
    }
   ],
   "source": [
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tdrace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Here's what it looks like for a sentence\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"All blue brains doth right the righteous king\"\n",
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All  :  all\n",
      "blue  :  blue\n",
      "brains  :  brain\n",
      "doth  :  doth\n",
      "right  :  right\n",
      "the  :  the\n",
      "righteous  :  righteou\n",
      "king  :  king\n"
     ]
    }
   ],
   "source": [
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF on random text data.\n",
    "# following tutorial here:\n",
    "# https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8\n",
    "\n",
    "text1 = \"All blue brains doth right the righteous king. Here lies sad and blighted boy Rupert. Ring around the rosie and pockets full of posies, and so forth!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove special characters\n",
    "\n",
    "def remove_string_special_characters(s):\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "    stripped = stripped.strip()\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split the text into sentences, calculating word count of each.\n",
    "# Each sentence is a document.\n",
    "\n",
    "def get_doc(sent):\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1\n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id': i, 'doc_length': count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return total number of words in the text\n",
    "\n",
    "def count_words(sent):\n",
    "    count = 0\n",
    "    wirds = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a frequency dictionary of each word in each document\n",
    "\n",
    "def create_freq_dict(sents):\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word] += 1\n",
    "            else:\n",
    "                freq_dict[word] = 1\n",
    "            temp = {'doc_id': i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the tf, frequency of a term in the document divided by\n",
    "# the total number of terms in the document\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores = []\n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id': id,\n",
    "                   'TF_score': tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],\n",
    "                   'key': k}\n",
    "            TF_scores.append(temp)\n",
    "        return TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute idf\n",
    "\n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id': counter, 'IDF_score': math.log(len(doc_info)/count), 'key': k}\n",
    "            \n",
    "            IDF_scores.append(temp)\n",
    "            \n",
    "        return IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute TF-IDF\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id': j['doc_id'],\n",
    "                       'TFIDF_score': j['IDF_score']*i['TF_score'],\n",
    "                       'key': i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sents = sent_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sents_clean = [remove_string_special_characters(s) for s in text_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_info = get_doc(text_sents_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDict_list = create_freq_dict(text_sents_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_scores = computeTF(doc_info, freqDict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_scores = computeIDF(doc_info, freqDict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1, 'doc_length': 10},\n",
       " {'doc_id': 2, 'doc_length': 10},\n",
       " {'doc_id': 3, 'doc_length': 10}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1,\n",
       "  'freq_dict': {'all': 1,\n",
       "   'blue': 1,\n",
       "   'brains': 1,\n",
       "   'doth': 1,\n",
       "   'right': 1,\n",
       "   'the': 1,\n",
       "   'righteous': 1,\n",
       "   'king': 1}},\n",
       " {'doc_id': 2,\n",
       "  'freq_dict': {'here': 1,\n",
       "   'lies': 1,\n",
       "   'sad': 1,\n",
       "   'and': 1,\n",
       "   'blighted': 1,\n",
       "   'boy': 1,\n",
       "   'rupert': 1}},\n",
       " {'doc_id': 3,\n",
       "  'freq_dict': {'ring': 1,\n",
       "   'around': 1,\n",
       "   'the': 1,\n",
       "   'rosie': 1,\n",
       "   'and': 2,\n",
       "   'pockets': 1,\n",
       "   'full': 1,\n",
       "   'of': 1,\n",
       "   'posies': 1,\n",
       "   'so': 1,\n",
       "   'forth': 1}}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqDict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1, 'TF_score': 0.1, 'key': 'all'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'blue'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'brains'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'doth'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'right'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'the'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'righteous'},\n",
       " {'doc_id': 1, 'TF_score': 0.1, 'key': 'king'}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'all'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'blue'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'brains'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'doth'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'right'},\n",
       " {'doc_id': 1, 'IDF_score': 0.4054651081081644, 'key': 'the'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'righteous'},\n",
       " {'doc_id': 1, 'IDF_score': 1.0986122886681098, 'key': 'king'}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'all'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'blue'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'brains'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'doth'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'right'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.04054651081081644, 'key': 'the'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'righteous'},\n",
       " {'doc_id': 1, 'TFIDF_score': 0.10986122886681099, 'key': 'king'}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding dictionaries of features\n",
    "\n",
    "# Create dictionary\n",
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "             {\"Red\": 4, \"Blue\": 3},\n",
    "             {\"Red\": 1, \"Yellow\": 2},\n",
    "             {\"Red\": 2, \"Yellow\": 2}]\n",
    "\n",
    "# Create dictionary vectorizer\n",
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Convert dictionary to feature matrix\n",
    "features = dictvectorizer.fit_transform(data_dict)\n",
    "\n",
    "# View feature matrix\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = dictvectorizer.get_feature_names()\n",
    "\n",
    "# View feature names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from features\n",
    "pd.DataFrame(features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word counts dictionaries for four documents\n",
    "doc_1_word_count = {\"Red\": 2, \"Blue\": 4}\n",
    "doc_2_word_count = {\"Red\": 4, \"Blue\": 3}\n",
    "doc_3_word_count = {\"Red\": 1, \"Yellow\": 2}\n",
    "doc_4_word_count = {\"Red\": 2, \"Yellow\": 2}\n",
    "\n",
    "# Create list\n",
    "doc_word_counts = [doc_1_word_count,\n",
    "                   doc_2_word_count,\n",
    "                   doc_3_word_count,\n",
    "                   doc_4_word_count]\n",
    "\n",
    "# Convert list of word count dictionaries into feature matrix\n",
    "dictvectorizer.fit_transform(doc_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing text\n",
    "\n",
    "# Create text\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# Tokenize words\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "# Tokenize sentences\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tdrace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagging parts of speech\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# Use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# Show parts of speech\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter words\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "          \"Political science is an amazing field\",\n",
    "          \"San Francisco is an awesome city\"]\n",
    "\n",
    "# Create list\n",
    "tagged_tweets = []\n",
    "\n",
    "# Tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "# Use one-hot encoding to convert the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show feature names\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\tdrace\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "# Get some text from the Brown Corpus, broken into sentences\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Split into 4000 sentences for training and 623 for testing\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# Create backoff tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# Show accuracy\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
